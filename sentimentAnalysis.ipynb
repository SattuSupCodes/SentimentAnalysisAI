{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaw8km8gQOdS6Wa+lSh76h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SattuSupCodes/SentimentAnalysisAI/blob/main/sentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "10T0YSHO4rsN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing dataset\n",
        "df = pd.read_csv(\"sentiment.csv\", encoding=\"latin1\")\n",
        "\n",
        "df = df[['text', 'sentiment']]\n",
        " #dropping themm null itemss\n",
        "df = df.dropna(subset=['text', 'sentiment'])"
      ],
      "metadata": {
        "id": "JUPutZlK5avG"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"dataset shape:\" , df.shape)\n",
        "print(\"\\nsentiment distribution:\\n\", df['sentiment'].value_counts())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RCmslAHS5o2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d44a8c-4aea-4e0c-8596-cc8ffa26157f"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset shape: (3534, 2)\n",
            "\n",
            "sentiment distribution:\n",
            " sentiment\n",
            "neutral     1430\n",
            "positive    1103\n",
            "negative    1001\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning time\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # remove urls\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)            # remove @ mentions\n",
        "    text = re.sub(r\"#[A-Za-z0-9_]+\", \"\", text)  # remove hashtags\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)     # remove emojis & punctuation\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "9_lQsWMn6sx5"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[\"clean_text\"]\n",
        "y = df[\"sentiment\"]\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_int = label_encoder.fit_transform(y_train)\n",
        "y_test_int = label_encoder.transform(y_test) #we are tryna make things harder for us"
      ],
      "metadata": {
        "id": "exQjjkEZ7AMZ"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(label_encoder.classes_)\n",
        "print(\"Classes:\", label_encoder.classes_)\n",
        "print(\"Encoded Labels (Train):\", y_train_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wMYfodkGIuu",
        "outputId": "18e3b71b-6271-44a5-fc8e-76c82bb80d83"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['negative' 'neutral' 'positive']\n",
            "Encoded Labels (Train): [0 0 1 ... 0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=6000, ngram_range=(1,2))\n",
        "x_test_vec = vectorizer.fit_transform(x_test)\n",
        "x_train_vec = vectorizer.transform(x_train)\n",
        "x_train_dense = x_train_vec.toarray()\n",
        "x_test_dense = x_test_vec.toarray()\n",
        "\n",
        "num_samples, num_features = x_train_dense.shape\n",
        "print(\"Train shape:\", x_train_dense.shape)\n",
        "print(\"Test shape:\", x_test_dense.shape)"
      ],
      "metadata": {
        "id": "QPad4qq08JBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1831a697-465a-41da-a34c-770855e5b445"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (2827, 6000)\n",
            "Test shape: (707, 6000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_one_hot(y_int, num_classes):\n",
        "  N = len(y_int)\n",
        "  one_hot = np.zeros((N, num_classes))\n",
        "  one_hot[np.arange(N), y_int] = 1\n",
        "  return one_hot\n",
        "y_train_onehot = to_one_hot(y_train_int, num_classes)\n",
        "y_test_onehot = to_one_hot(y_test_int, num_classes)\n",
        "\n",
        "print(\"Train shape:\", y_train_onehot.shape)\n",
        "print(\"Test shape:\", y_test_onehot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWDacLyeH2KU",
        "outputId": "037e7a61-b08b-45d4-b5bc-8cfc5273ca8a"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (2827, 3)\n",
            "Test shape: (707, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "one hot basically turns your positive, negative, neutral into matrices\n",
        "\n",
        "negative = [1 0 0],\n",
        "neutral = [0 1 0],\n",
        "positive = [ 0 0 1]"
      ],
      "metadata": {
        "id": "dFTCADvMIaPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets initialise weights and use softmax and feel a bit more engineer smh smh\n",
        "\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(num_features, num_classes) * 0.01\n",
        "b = np.zeros((1, num_classes))"
      ],
      "metadata": {
        "id": "cBLqTRZ5JGDR"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "  exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "  return exps / np.sum(exps, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "ho6aZBgUJirh"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import log_softmax\n",
        "#model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
        "#model.fit(x_train_vec, y_train)\n",
        "\n",
        "#letus open the .fit() cuz why not\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  logits = np.dot(x_train_dense, W) + b\n",
        "  probs = softmax(logits)\n",
        "\n",
        "  epsilon = 1e-15 #to avoid log0\n",
        "  loss = -np.mean(np.sum(y_train_onehot * np.log(probs + epsilon), axis=1))\n",
        "  error = (probs - y_train_onehot) / num_samples\n",
        "  dW = np.dot(x_train_dense.T, error)\n",
        "  db = np.sum(error, axis=0, keepdims=True)\n",
        "\n",
        "  #updating weights\n",
        "\n",
        "  W -= learning_rate * dW\n",
        "  b -= learning_rate * db\n",
        "\n",
        "  if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LcpiJelP8dmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4021d159-8801-4403-8332-cc962c0454cf"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Loss: 1.0985\n",
            "Epoch 10/100 - Loss: 1.0922\n",
            "Epoch 20/100 - Loss: 1.0882\n",
            "Epoch 30/100 - Loss: 1.0858\n",
            "Epoch 40/100 - Loss: 1.0842\n",
            "Epoch 50/100 - Loss: 1.0830\n",
            "Epoch 60/100 - Loss: 1.0820\n",
            "Epoch 70/100 - Loss: 1.0811\n",
            "Epoch 80/100 - Loss: 1.0802\n",
            "Epoch 90/100 - Loss: 1.0793\n",
            "Epoch 100/100 - Loss: 1.0785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch is the training session and logits is the AI thinking in a nutshell then giving output based on highest probability"
      ],
      "metadata": {
        "id": "BH2a5JElPfLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward on test data\n",
        "logits_test = np.dot(x_test_dense, W) + b\n",
        "y_test_pred_proba = softmax(logits_test)\n",
        "\n",
        "# Class with highest probability\n",
        "y_test_pred_int = np.argmax(y_test_pred_proba, axis=1)\n",
        "\n",
        "# Compare with true labels\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Scratch Logistic Regression Accuracy:\",\n",
        "      accuracy_score(y_test_int, y_test_pred_int))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\",\n",
        "      classification_report(y_test_int, y_test_pred_int,\n",
        "                            target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UQ6tUEGj8qKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a9dc0b-0ea1-44a9-f16e-2fa7b0c2f521"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scratch Logistic Regression Accuracy: 0.33663366336633666\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.28      0.27      0.27       200\n",
            "     neutral       0.40      0.37      0.38       286\n",
            "    positive       0.31      0.36      0.33       221\n",
            "\n",
            "    accuracy                           0.34       707\n",
            "   macro avg       0.33      0.33      0.33       707\n",
            "weighted avg       0.34      0.34      0.34       707\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "  text = clean_text(text)\n",
        "  vec = vectorizer.transform([text]).toarray()\n",
        "  logits = np.dot(vec, W) + b\n",
        "  proba = softmax(logits)[0]\n",
        "  pred_int = np.argmax(proba)\n",
        "  label = label_encoder.inverse_transform([pred_int])[0]\n",
        "  return label, dict(zip(label_encoder.classes_,proba))"
      ],
      "metadata": {
        "id": "60cZ7JRo9Aq-"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nExamples:\")\n",
        "print(predict_sentiment(\"I am so happy today!!\"))\n",
        "print(predict_sentiment(\"This is the worst thing ever\"))\n",
        "print(predict_sentiment(\"It's okay, nothing special\"))"
      ],
      "metadata": {
        "id": "Ul1Dh2hb-X_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997c81f3-a5b8-4b62-8c48-9ccdd18d1497"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Examples:\n",
            "('positive', {'negative': np.float64(0.33028875981735883), 'neutral': np.float64(0.3310224035801897), 'positive': np.float64(0.3386888366024514)})\n",
            "('negative', {'negative': np.float64(0.3360399123607244), 'neutral': np.float64(0.3308234557644451), 'positive': np.float64(0.33313663187483045)})\n",
            "('neutral', {'negative': np.float64(0.32781830186693683), 'neutral': np.float64(0.33720446966698764), 'positive': np.float64(0.33497722846607547)})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets learn as we go\n",
        "\n",
        "TF-IDF = Term frequency x inverse document frequency\n",
        "\n",
        "TF: How frequently does this word appear in this tweet?\n",
        "IDF: How rare is this word in whole dataset?"
      ],
      "metadata": {
        "id": "-yJWSDcpArI5"
      }
    }
  ]
}